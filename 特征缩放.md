# 特征缩放



## 为什么进行特征缩放

​	一个由两个变量评估的数据：$y=\theta_0+\theta_1x_1+\theta_2x_2$ ，如果仅仅只是按照我们之前的代码，直接从(0,0,0)开始梯度下降，难免会让我们产生疑问：“这个初始点是否合适？有没有更高效的方式？”比如此处如果$x_1$ 范围为300-500，$x_2$ 范围为0-5，并且这两个变量对y的影响都是很可观的。那么你就会想 “ $x_1$ 的范围那么大，可见它乘上系数$\theta_1$ 后要产生一个很大的变化才能达到与 $x_2$ 变化1甚至0.1相同的结果。”这就让我们想到$\theta_1$ 需要是一个很小的值， 来削弱$x_1$ 值的变化。这个在吴恩达机器学习里有一个很好的例子(不要犹豫，肯定是和房价有关的东西)
$$
\begin{aligned}
&price=w_1x_1+w_2x_2+b&\\
&house: x_1=2000,x_2=5,price=500k\\
&w_1=50,w_2=0.1,b=50\Rightarrow price=100,050k(\text{效果不好})\\
&w_1 = 0.1,w_2 = 50,b = 50\Rightarrow price = 500k(\text{效果很好})\\
\end{aligned}
$$
​	我们自然而然地想到：如果钱的单位是元，我们就把它变成千元；如过人数的单位是个，就把它变成千个万个，将所有的数据通过这种简单的单位调配使其达到差不多的值。

​	但是别忘了我们通常输入的都是零向量，也就是说如果可以把所有数据通过简单的线性或加减变换使其都在0的某一个领域内，那就再好不过了。

## 特征缩放的数学原理

### 方法一

​	假设$x_1$ 已经收集到了$m$ 组数据，那么可以这样进行特征缩放：
$$
\text{取平均值: }\text{ } u_1=\frac{1}{m}\sum_{i=1}^{m}x_1^{(i)}\\
\text{取最大值和最小值: }\text{ } max\{x_1\}\quad min\{x_1\} \\
\text{令: }\text{ } x_1:=\frac{x_1-u_1}{max\{x_1\}- min\{x_1\}}
$$

### 方法二

$$
\text{取平均值，标准差: }\text{ } u_1 \quad \sigma_1\\
\text{令: }\text{ } x_1:=\frac{x_1-u_1}{\sigma_1}
$$

​	特征放缩之前，舍弃明显不对劲的数据尤为重要，这个在中学生物实验就已经提到过了。

### 将$\theta$ 数组变换回来

​	我们无非就是运行了以下的式子：
$$
\frac{y-u_y}{\sigma_y} = \theta_0^{'}+\theta_1^{'}\frac{x_1-u_1}{\sigma_1}+\theta_2^{'}\frac{x_2-u_2}{\sigma_2}+\cdots+\theta_n^{'}\frac{x_n-u_n}{\sigma_n}
$$
这里“$\theta_i'$”就是我们运行出来的，经过了特征缩放后算出的数据，而原式为：
$$
y=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_n x_n
$$
联立可以得出：
$$
\theta_0=(\theta_0^{'}-\sum_{i=1}^{n}\frac{\theta_i\times u_i}{\sigma_i})\times \sigma_y\\ \\ \theta_i=\frac{\theta_i^{'} }{\sigma_i}\sigma_y+u_y \text{ }\text{ }\text{ } (i=1,2,3\cdots n)
$$
## 代码实现
首先导入要用的库和数据
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import  Axes3D

path = 'ex1data2.txt'
data = pd.read_csv(path,header=None,names=['Size','Bedrooms','Price'])
data.head() #查看数据有多少列，方便定义theta_begin
```

对数据进行特征缩放
```python
#处理数据
means = np.array(data.mean().values).reshape(1,3) #把平均值保存下来并且化为array类型
std = np.array(data.std().values).reshape(1,3) #保存mean std

temp = data.copy() #保存源数据
data = (data - data.mean(axis=0))/data.std(axis=0) #data.mean(axis=0)表示每列替换为平均值
print(data.head())

data.insert(0,'ones',1)
cols = data.shape[1]
X = data.iloc[:, 0:cols - 1]  # X是所有行去掉最后一列
Y = data.iloc[:, cols - 1:cols]

X = np.array(X.values)
Y = np.array(Y.values)
theta_begin = np.array([0, 0,0]).reshape(1, 3)  # 调整为(1,3)形状以匹配原矩阵形状
```
代价函数和梯度下降函数还是老样子，因为我们之前写的函数对于任意列数都有用
```python
iters=1000
cost=np.zeros(iters)

# 代价函数和梯度下降函数
def cost_function(X, Y, theta):
    inner = np.power((X @ theta.T) - Y, 2)
    return np.sum(inner) / (2 * len(X))
def gradient_descent(X, y, theta, alpha, iters):
    temp = np.zeros(theta.shape)
    parameters = theta.ravel().shape[0]
    global cost

    for i in range(iters):
        error = (X @ theta.T) - y

        for j in range(parameters):
            term = np.multiply(error, X[:, j].reshape(-1, 1))
            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))

        theta = temp.copy()
        cost[i] = cost_function(X, Y, theta)

    return theta
```
我们查看特征缩放得出来的$\theta^{'}$ 数组，由于特征缩放不影响cost的评估，我们再可视化一下cost
```python
g = gradient_descent(X,Y,theta_begin,0.01,iters)
print(g)
print(cost_function(X,Y,g))

# 迭代cost可视化
fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(np.arange(iters), cost, 'r')
ax.set_xlabel('iters')
ax.set_ylabel('cost')
ax.set_title('change of cost')
plt.show()
```
我们接下来将参数转换回去，即从$\theta^{'}$ 到$\theta$
```python
def theta_back(theta,means,std):
    thetaback = theta.copy()

    thetaback[0,0] = (theta[0,0]-(np.sum((means[0,:-1]*theta[0,1:])/std[0,:-1])))*std[0,-1]+means[0,-1] #目标变量也被标准化

    thetaback[0,1:] = (theta[0,1:]/std[0,:-1])*std[0,-1]
    return thetaback
```
我们看看最终结果
```python
gf = theta_back(g,means,std)
print(gf)
```
```python
[[88307.21151185   138.22534685 -7709.05876589]]
```