# 线性回归



## 数学原理

#### 模型表达

​	我们统计了某个连锁店在不同人口城市的年利润值，例如在6.1101万人的城市，年利润可以达到17.5920万人民币。现在我要在一个新的城市开这么一家连锁店，人口可以查到，希望预估一下我的年利润值。

​	（这里直接使用吴恩达课程的数据表，你问**Fitten**它也是差不多的例子，你也可以直接去看GitHub上其他人的笔记，只不过我这里几乎不再使用Numpy中的matrix类型，而是依赖于array来实现线性代数的工作。下载Watt Toolkit打开GitHub加速是最直接的加速GitHub的方法）

​	我们先来回忆高中数学，好像确实学过这个东西，只不过今天我们要利用高等数学工具。

​	模型自然是一元线性方程 $y=wx+b$ ，我们要做的就是调节$w,x$让它适合我们的数据点

​	![让线适合数据点](../pycharm/mathematicmodel/机器学习/Figure_2.png)

​	我们在高中天天用那个公式算(现在看来高中简直是我们最快乐的时光)，现在我们要利用高等数学的知识找到我们的$w$和$b$.

​	首先我们来评估方程的误差，很显然用高中的方差就可以评估拟合的效果。

​	此处我们讨论更一般的情形：$f(x) = \theta_0 +\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n$ 表示有n个变量决定函数最终的值，$\theta_0$为截距，$x$是一个n维向量,里面是各个自变量，每一个数据对应的误差为$f(x)-y$。

​	如果我们已经收集到了$m$组数据，用上角标来标记它们，那么第 $i$组数据的误差为$f(x^{(i)})^-y^{(i)}$，现在我们要反过来以向量$\theta=\begin{pmatrix}\theta_0&\theta_1&\theta_2&\cdots&\theta_n\end{pmatrix}^T$为自变量，讨论什么时候误差最小，我们定义**代价函数：**
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(f(x^{(i)})-y^{(i)})^2
$$
​	这似乎就是方差除以了2，虽然我们知道除以2并不影响误差的评估效果，但为什么不直接使用方差呢？这其实是因为我们要求导，平方项会有一个2放下来。

#### 梯度下降

​	这是我们要利用的新知识，但让我们先看看梯度下降的原理：在纸面上画一个二次函数，任取一个点$(x_0,y_0)$, 计算它在这一个点的导数值，并作出其切线，取一个微小的步长$\alpha>0$ ,让这个点按照$x^{'}_0=x_0-\alpha f^{'}(x_0)$ 移动

![](C:/Users/s1557/Pictures/Screenshots/屏幕截图 2025-04-06 095618.png)



​	你会发现它总会朝着最低点方向移动，这就是梯度下降的原理，前提是步长$\alpha$ 不能太大，一旦太大，就不是趋近于最小值了。我们把步长$\alpha$ 称为**学习率**。

​	敏锐的同学肯定注意到，我们可以以任何一个点为起点，进行梯度下降，但是我们取到的永远是局部最小值，也就是极小值。只不过对于那些能够很好拟合成直线的数据，其三维图都是碗状的，就像二维里面的二次函数，会趋于同一个最小值。

​	我们将式子$x^{'}_0=x_0-\alpha f^{'}(x_0)$ 推广到一般形式，这里就直接用计算机赋值的表达方式。每一次操作称为一次迭代。
$$
\theta_j=\theta_j-\alpha \frac{\partial J(\theta)}{\partial \theta_j}\\
其中\frac{\partial J(\theta)}{\partial \theta_j}=\frac{1}{m}\sum_{i=1}^{m}(f(x^{(i)})-y^{(i)})x_j^{(i)}
$$
​	这个线性函数求偏导当然不在话下，但这里有一个代码编写中需要注意的点，这个公式毫无疑问要用到原来的$\theta$ 向量，所有的$\theta_i$用的都是同一个$\partial J(\theta)$，但是当你改变了$\theta_0$ 后，原来的$\theta$ 向量已经发生了改变 ,这就导致了计算不同步，所以在循环体外部要定义一个temp来储存原向量，用temp去进行运算。这个在代码编写过程中，会很容易get到。

## 代码编写

首先我们按照惯例导入要用的库

`````python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sympy.abc import theta
`````

在GitHub，Gitee或者课程上下载第一周的文件，请注意要将文件移动到代码文件的同一个文件夹里。

```python
# 用pandas读取数据并保存在变量data中
path = 'ex1data1.txt'
# 文件并没有给列命名，所以header=None,我们将数据命名为Population,Profit
data = pd.read_csv(path,header = None,names = ['Population','Profit'])

# 先来看一下散点图(scatter)
data.plot(kind='scatter',x='Population',y='Profit',figsize=(12,8))
```

接下来就是编写代价函数了，在这个函数里，我们需要传入自变量数组x，真实值数组y，以及我们要计算的$\theta$ 数组。我们后续把theta定义为一维的行数组，所以进行对应元素相乘前要用到转置(.T方法)

```python
def cost_function(X,Y,theta):
    inner = np.power((X @ theta.T)-Y,2) # 表示对应元素相乘，此处inner也是数组
    return np.sum(inner)/(2*len(X))
```

接下来我们把txt文件中的数据拆分，使之成为我们需要的几个数组。

```python
#现在最左端添加一列1，这样就无需进行显式的加法操作表达截距
data.insert(0,'ones',1)
col = data.shape[1] #计算有多少列，shape[]中0和1分别表示统计行数和统计列数
X = data.iloc[:,0:col-1] #用iloc方法取所有行，舍弃最后一列
Y = data.iloc[:,col] #取最后一列
theta_begin = np.array([0,0]).reshape(1,2) #此段代码以及放弃matrix，所以注意用reshape匹配原矩阵形状
#检查数组形状
print(X.shape)
print(theta_begin.shape)
print(Y.shape)
```

下面我们来实现梯度下降函数，我们不可能让偏导数为0的时候才停下来，计算机很难算出标准的0，这里我们指定一个迭代次数iters，显然迭代iters次有一个循环，我们要改变$\theta$ 数组里所有的n+1个元素又需要一个循环

```python
def gradientDescent(X, y, theta, alpha, iters): #iters传入迭代次数 
    temp = np.zeros(theta.shape) #我们不能在第二个循环里直接改变theta的值，创建一个temp储存迭代后的数据
    parameters = theta.ravel().shape[0] #.ravel平摊数组统计theta参数个数，这就是第二个循环的循环次数
    #我们需要第一个循环，来进行iters次迭代
    for i in range(iters):
        error = (X @ thete) - y # 每一次的误差都是不一样的，误差写在在第二个循环前
        for j in range(parameters):
            term = np.multiply(error,X[:,j].reshape(-1,1)) #第二个循环每一次都在改变角标为j的theta的值
            temp[0,j] = theta[0,j] - (alpha/len(X)) * np.sum(term) #这里temp就发挥了存储作用
            
        theta = temp.copy() #跳出第二个循环后将theta值统一改变
        
    return theta #返回theta数组

alpha = 0.01
iters = 1000
g = def gradientDescent(X,y,theta,alpha,iters)
print(g)
```

现在我们来可视化数据

```python
x = np.linspace(data.Population.min(), data.Population.max(), 100) #横坐标
f = g[0, 0] + (g[0, 1] * x) #拟合的直线

fig, ax = plt.subplots(figsize=(12, 8)) #创建图形对象fig，轴对象ax
ax.plot(x, f, 'r', label='Prediction') #直线
ax.scatter(data.Population, data.Profit, label='Training Data') #散点
ax.legend(loc=2) #添加图例。loc=2,左上方
ax.set_xlabel('Population') #横轴标签
ax.set_ylabel('Profit') #纵轴标签
ax.set_title('Predicted Profit vs. Population Size') #标题
plt.show() #显示
```

![](../pycharm/mathematicmodel/机器学习/Figure_2.png)

我们想看看代价函数的变化怎么弄呢？

```python
#在全局定义一个cost变量，用以记录代价函数的值,注意放在iters = 1000的后面
cost = np.zeros(iters)
```

将梯度下降函数修改为

```python
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.zeros(theta.shape)
    parameters = theta.ravel().shape[0]
    global cost

    for i in range(iters):
        error = (X @ theta.T) - y

        for j in range(parameters):
            term = np.multiply(error, X[:, j].reshape(-1, 1))
            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))

        theta = temp.copy()
        cost[i] = cost_function(X, Y, theta)

    return theta
```

接下来我们可视化代价函数图像

```python
fig2, ax2 = plt.subplots(figsize=(12, 8))
ax2.plot(np.arange(iters), cost, 'r')
ax2.set_xlabel('Iterations')
ax2.set_ylabel('Cost')
ax2.set_title('costchange')
plt.show()
```

![](../pycharm/mathematicmodel/机器学习/Figure_1.png)







